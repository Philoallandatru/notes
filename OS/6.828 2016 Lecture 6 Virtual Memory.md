

# 6.828 Lecture 2 Notes: x86 and PC architecture

## Outline

-   PC architecture
-   x86 instruction set
-   gcc calling conventions
-   PC emulation

## PC architecture

-   A full PC has:

    -   an x86 CPU with registers, execution unit, and memory management
    -   CPU chip pins include address and data signals
    -   memory
    -   disk
    -   keyboard
    -   display
    -   other resources: BIOS ROM, clock, ...

-   We will start with the original 16-bit 8086 CPU (1978)

-   CPU runs instructions:

    ```
    for(;;){
    	run next instruction
    }
    ```

-   Needs work space: registers

    -   four 16-bit data registers: AX, BX, CX, DX
    -   each in two 8-bit halves, e.g. AH and AL
    -   very fast, very few

-   More work space: memory

    -   CPU sends out address on address lines (wires, one bit per wire)
    -   Data comes back on data lines
    -   *or* data is written to data lines

-   Add address registers: pointers into memory

    -   SP - stack pointer
    -   BP - frame base pointer
    -   SI - source index
    -   DI - destination index

-   Instructions are in memory too!

    -   IP - instruction pointer (PC on PDP-11, everything else)
    -   increment after running each instruction
    -   can be modified by CALL, RET, JMP, conditional jumps

-   Want conditional jumps

    -   FLAGS - various condition codes
        -   whether last arithmetic operation overflowed
        -   ... was positive/negative
        -   ... was [not] zero
        -   ... carry/borrow on add/subtract
        -   ... etc.
        -   whether interrupts are enabled
        -   direction of data copy instructions
    -   JP, JN, J[N]Z, J[N]C, J[N]O ...

-   Still not interesting - need I/O to interact with outside world

    -   Original PC architecture: use dedicated

         

        I/O space

        -   Works same as memory accesses but set I/O signal

        -   Only 1024 I/O addresses

        -   Accessed with special instructions (IN, OUT)

        -   Example: write a byte to line printer:

            \#define DATA_PORT    0x378 #define STATUS_PORT  0x379 #define   BUSY 0x80 #define CONTROL_PORT 0x37A #define   STROBE 0x01 void lpt_putc(int c) {  /* wait for printer to consume previous byte */  while((inb(STATUS_PORT) & BUSY) == 0)    ;   /* put the byte on the parallel lines */  outb(DATA_PORT, c);   /* tell the printer to look at the data */  outb(CONTROL_PORT, STROBE);  outb(CONTROL_PORT, 0); } 

            ```
            		
            ```

    -   Memory-Mapped I/O

        -   Use normal physical memory addresses
            -   Gets around limited size of I/O address space
            -   No need for special instructions
            -   System controller routes to appropriate device
        -   Works like ``magic'' memory:
            -   *Addressed* and *accessed* like memory, but ...
            -   ... does not *behave* like memory!
            -   Reads and writes can have ``side effects''
            -   Read results can change due to external events

-   What if we want to use more than 2^16 bytes of memory?

    -   8086 has 20-bit physical addresses, can have 1 Meg RAM
    -   the extra four bits usually come from a 16-bit "segment register":
    -   CS - code segment, for fetches via IP
    -   SS - stack segment, for load/store via SP and BP
    -   DS - data segment, for load/store via other registers
    -   ES - another data segment, destination for string operations
    -   virtual to physical translation: pa = va + seg*16
    -   e.g. set CS = 4096 to execute starting at 65536
    -   tricky: can't use the 16-bit address of a stack variable as a pointer
    -   a *far pointer* includes full segment:offset (16 + 16 bits)
    -   tricky: pointer arithmetic and array indexing across segment boundaries

-   But 8086's 16-bit addresses and data were still painfully small

    -   80386 added support for 32-bit data and addresses (1985)
    -   boots in 16-bit mode, boot.S switches to 32-bit mode
    -   registers are 32 bits wide, called EAX rather than AX
    -   operands and addresses that were 16-bit became 32-bit in 32-bit mode, e.g. ADD does 32-bit arithmetic
    -   prefixes 0x66/0x67 toggle between 16-bit and 32-bit operands and addresses: in 32-bit mode, MOVW is expressed as 0x66 MOVW
    -   the .code32 in boot.S tells assembler to generate 0x66 for e.g. MOVW
    -   80386 also changed segments and added paged memory...

-   Example instruction encoding

    ```
    	b8 cd ab		16-bit CPU,  AX <- 0xabcd
    	b8 34 12 cd ab		32-bit CPU, EAX <- 0xabcd1234
    	66 b8 cd ab		32-bit CPU,  AX <- 0xabcd
    ```

## x86 Physical Memory Map

-   The physical address space mostly looks like ordinary RAM
-   Except some low-memory addresses actually refer to other things
-   Writes to VGA memory appear on the screen
-   Reset or power-on jumps to ROM at 0xfffffff0 (so must be ROM at top...)

```
+------------------+  <- 0xFFFFFFFF (4GB)
|      32-bit      |
|  memory mapped   |
|     devices      |
|                  |
/\/\/\/\/\/\/\/\/\/\

/\/\/\/\/\/\/\/\/\/\
|                  |
|      Unused      |
|                  |
+------------------+  <- depends on amount of RAM
|                  |
|                  |
| Extended Memory  |
|                  |
|                  |
+------------------+  <- 0x00100000 (1MB)
|     BIOS ROM     |
+------------------+  <- 0x000F0000 (960KB)
|  16-bit devices, |
|  expansion ROMs  |
+------------------+  <- 0x000C0000 (768KB)
|   VGA Display    |
+------------------+  <- 0x000A0000 (640KB)
|                  |
|    Low Memory    |
|                  |
+------------------+  <- 0x00000000
```

## x86 Instruction Set

-   Intel syntax: `op dst, src` (Intel manuals!)

-   AT&T (gcc/gas) syntax:

     

    op src, dst

     

    (labs, xv6)

    -   uses b, w, l suffix on instructions to specify size of operands

-   Operands are registers, constant, memory via register, memory via constant

-   Examples:

    | AT&T syntax        | "C"-ish equivalent        |                 |
    | ------------------ | ------------------------- | --------------- |
    | movl %eax, %edx    | edx = eax;                | *register mode* |
    | movl $0x123, %edx  | edx = 0x123;              | *immediate*     |
    | movl 0x123, %edx   | edx = *(int32_t*)0x123;   | *direct*        |
    | movl (%ebx), %edx  | edx = *(int32_t*)ebx;     | *indirect*      |
    | movl 4(%ebx), %edx | edx = *(int32_t*)(ebx+4); | *displaced*     |

-   Instruction classes

    -   data movement: MOV, PUSH, POP, ...
    -   arithmetic: TEST, SHL, ADD, AND, ...
    -   i/o: IN, OUT, ...
    -   control: JMP, JZ, JNZ, CALL, RET
    -   string: REP MOVSB, ...
    -   system: IRET, INT

-   Intel architecture manual Volume 2 is *the* reference

## gcc x86 calling conventions

-   x86 dictates that stack grows down:

    | Example instruction | What it does                           |
    | ------------------- | -------------------------------------- |
    | pushl %eax          | subl $4, %esp movl %eax, (%esp)        |
    | popl %eax           | movl (%esp), %eax addl $4, %esp        |
    | call 0x12345        | pushl %eip (*) movl $0x12345, %eip (*) |
    | ret                 | popl %eip (*)                          |

    (*)

     

    Not real instructions

-   GCC dictates how the stack is used. Contract between caller and callee on x86:

    -   at entry to a function (i.e. just after call):
        -   %eip points at first instruction of function
        -   %esp+4 points at first argument
        -   %esp points at return address
    -   after ret instruction:
        -   %eip contains return address
        -   %esp points at arguments pushed by caller
        -   called function may have trashed arguments
        -   %eax (and %edx, if return type is 64-bit) contains return value (or trash if function is `void`)
        -   %eax, %edx (above), and %ecx may be trashed
        -   %ebp, %ebx, %esi, %edi must contain contents from time of `call`
    -   Terminology:
        -   %eax, %ecx, %edx are "caller save" registers
        -   %ebp, %ebx, %esi, %edi are "callee save" registers

-   Functions can do anything that doesn't violate contract. By convention, GCC does more:

    -   each function has a stack frame marked by %ebp, %esp

        ```
        		       +------------+   |
        		       | arg 2      |   \
        		       +------------+    >- previous function's stack frame
        		       | arg 1      |   /
        		       +------------+   |
        		       | ret %eip   |   /
        		       +============+   
        		       | saved %ebp |   \
        		%ebp-> +------------+   |
        		       |            |   |
        		       |   local    |   \
        		       | variables, |    >- current function's stack frame
        		       |    etc.    |   /
        		       |            |   |
        		       |            |   |
        		%esp-> +------------+   /
        		
        ```

    -   %esp can move to make stack frame bigger, smaller

    -   %ebp points at saved %ebp from previous function, chain to walk stack

    -   function prologue:

        ```assembly
        			pushl %ebp
        			movl %esp, %ebp
        		
        ```

        or

        ```assembly
        			enter $0, $0
        		
        ```

        enter usually not used: 4 bytes vs 3 for pushl+movl, not on hardware fast-path anymore

    -   function epilogue can easily find return EIP on stack:

        ```assembly
        			movl %ebp, %esp
        			popl %ebp
        		
        ```

        or

        ```assembly
        			leave
        		
        ```

        leave used often because it's 1 byte, vs 3 for movl+popl

-   Big example:

    -   C code

        ```c
        		int main(void) { return f(8)+1; }
        		int f(int x) { return g(x); }
        		int g(int x) { return x+3; }
        		
        ```

    -   assembler

        ```c
        		_main:
        					prologue
        			pushl %ebp
        			movl %esp, %ebp
        					body
        			pushl $8
        			call _f
        			addl $1, %eax
        					epilogue
        			movl %ebp, %esp
        			popl %ebp
        			ret
        		_f:
        					prologue
        			pushl %ebp
        			movl %esp, %ebp
        					body
        			pushl 8(%esp)
        			call _g
        					epilogue
        			movl %ebp, %esp
        			popl %ebp
        			ret
        
        		_g:
        					prologue
        			pushl %ebp
        			movl %esp, %ebp
        					save %ebx
        			pushl %ebx
        					body
        			movl 8(%ebp), %ebx
        			addl $3, %ebx
        			movl %ebx, %eax
        					restore %ebx
        			popl %ebx
        					epilogue
        			movl %ebp, %esp
        			popl %ebp
        			ret
        		
        ```

-   Super-small

     

    _g

    :

    ```assembly
    		_g:
    			movl 4(%esp), %eax
    			addl $3, %eax
    			ret
    	
    ```

-   Shortest `_f`?

-   Compiling, linking, loading:

    -   *Preprocessor* takes C source code (ASCII text), expands #include etc, produces C source code
    -   *Compiler* takes C source code (ASCII text), produces assembly language (also ASCII text)
    -   *Assembler* takes assembly language (ASCII text), produces `.o` file (binary, machine-readable!)
    -   *Linker* takes multiple '`.o`'s, produces a single *program image* (binary)
    -   *Loader* loads the program image into memory at run-time and starts it executing

## PC emulation

-   The Bochs emulator works by

    -   doing exactly what a real PC would do,
    -   only implemented in software rather than hardware!

-   Runs as a normal process in a "host" operating system (e.g., Linux)

-   Uses normal process storage to hold emulated hardware state: e.g.,

    -   Stores emulated CPU registers in global variables

        ```c
        		int32_t regs[8];
        		#define REG_EAX 1;
        		#define REG_EBX 2;
        		#define REG_ECX 3;
        		...
        		int32_t eip;
        		int16_t segregs[4];
        		...
        		
        ```

    -   Stores emulated physical memory in Boch's memory

        ```c
                        char mem[256*1024*1024];
                        
        ```

-   Execute instructions by simulating them in a loop:

    ```c
    	for (;;) {
    		read_instruction();
    		switch (decode_instruction_opcode()) {
    		case OPCODE_ADD:
    			int src = decode_src_reg();
    			int dst = decode_dst_reg();
    			regs[dst] = regs[dst] + regs[src];
    			break;
    		case OPCODE_SUB:
    			int src = decode_src_reg();
    			int dst = decode_dst_reg();
    			regs[dst] = regs[dst] - regs[src];
    			break;
    		...
    		}
    		eip += instruction_length;
    	}
    	
    ```

-   Simulate PC's physical memory map by decoding emulated "physical" addresses just like a PC would:

    ```c
    	#define KB		1024
    	#define MB		1024*1024
    
    	#define LOW_MEMORY	640*KB
    	#define EXT_MEMORY	10*MB
    
    	uint8_t low_mem[LOW_MEMORY];
    	uint8_t ext_mem[EXT_MEMORY];
    	uint8_t bios_rom[64*KB];
    
    	uint8_t read_byte(uint32_t phys_addr) {
    		if (phys_addr < LOW_MEMORY)
    			return low_mem[phys_addr];
    		else if (phys_addr >= 960*KB && phys_addr < 1*MB)
    			return rom_bios[phys_addr - 960*KB];
    		else if (phys_addr >= 1*MB && phys_addr < 1*MB+EXT_MEMORY) {
    			return ext_mem[phys_addr-1*MB];
    		else ...
    	}
    
    	void write_byte(uint32_t phys_addr, uint8_t val) {
    		if (phys_addr < LOW_MEMORY)
    			low_mem[phys_addr] = val;
    		else if (phys_addr >= 960*KB && phys_addr < 1*MB)
    			; /* ignore attempted write to ROM! */
    		else if (phys_addr >= 1*MB && phys_addr < 1*MB+EXT_MEMORY) {
    			ext_mem[phys_addr-1*MB] = val;
    		else ...
    	}
    	
    ```

-   Simulate I/O devices, etc., by detecting accesses to "special" memory and I/O space and emulating the correct behavior: e.g.,

    -   Reads/writes to emulated hard disk transformed into reads/writes of a file on the host system
    -   Writes to emulated VGA display hardware transformed into drawing into an X window
    -   Reads from emulated PC keyboard transformed into reads from X input event queue

# 6.828 2018 Lecture 4: Shell & OS organization

## Lecture Topic:
*   kernel system call API
    *   both details and design
            isolation, multiplexing, and sharing
    *   illustrate via shell and homework 2

## Overview Diagram

*   user / kernel
*   process = address space +  thread(s)
    *   process is a running program
*   app -> ```printf()``` -> write() -> SYSTEM CALL -> sys_write() -> ...
*     user-level libraries are app's private business

*    kernel internal functions are not callable by user
*     xv6 has a few dozen system calls; Linux a few hundred
*     details today are mostly about UNIX system-call API
    *   ​    basis for xv6, Linux, OSX, POSIX standard, &c
    *   ​    jos has very different system-calls; you'll build UNIX calls over jos

Homework solution

* Let's review Homework 2 (sh.c)
  * ```exec```
    why two execv() arguments?
    what happens to the arguments?
    what happens when exec'd process finishes?
    can execv() return?
    how is the shell able to continue after the command finishes?
  * redirect
    how does exec'd process learn about redirects? [kernel fd tables]
    does the redirect (or error exit) affect the main shell?
  * pipe
    ls | wc -l
    what if ls produces output faster than wc consumes it?
    what if ls is slower than wc?
    how does each command decide when to exit?
    what if reader didn't close the write end? [try it]
    what if writer didn't close the read end?
    how does the kernel know when to free the pipe buffer?

  * how does the shell know a pipeline is finished?
    e.g. ls | sort | tail -1

  * what's the tree of processes?
    sh parses as: ls | (sort | tail -1)
          sh
          sh1
      ls      sh2
          sort   tail

  * does the shell need to fork so many times?
    - what if sh didn't fork for pcmd->left? [try it]
      i.e. called runcmd() without forking?
    - what if sh didn't fork for pcmd->right? [try it]
      would user-visible behavior change?
      sleep 10 | echo hi

  * why wait() for pipe processes only after both are started?
    what if sh wait()ed for pcmd->left before 2nd fork? [try it]
      ls | wc -l
      cat < big | wc -l

  * the point: the system calls can be combined in many ways
    to obtain different behaviors.

Let's look at the challenge problems

 * How to implement sequencing with ";"?
   gcc sh.c ; ./a.out
   echo a ; echo b
   why wait() before scmd->right? [try it]

 * How to implement "&"?
   
   ```
   $ sleep 5 & 
   $ wait
```
   
   the implementation of & and wait is in main -- why?
   What if a background process exits while sh waits for a foreground process?
   
 * How to implement nesting?
   $ (echo a; echo b) | wc -l
   my ( ... ) implementation is only in sh's parser, not runcmd()
   it's neat that sh pipe code doesn't have to know it's applying to a sequence

 * How do these differ? 
   echo a > x ; echo b > x
   ( echo a ; echo b ) > x
   what's the mechanism that avoids overwriting?

## UNIX system call observations

* The fork/exec split looks wasteful -- fork() copies mem, exec() discards.
  why not e.g. pid = forkexec(path, argv, fd0, fd1) ?
  the fork/exec split is useful:
    fork(); I/O redirection; exec()
      or fork(); complex nested command; exit.
      as in ( cmd1 ; cmd2 ) | cmd3
    fork() alone: parallel processing
    exec() alone: /bin/login ... exec("/bin/sh")
  fork is cheap for small programs -- on my machine:
    fork+exec takes 400 microseconds (2500 / second)
    fork alone takes 80 microseconds (12000 / second)
    some tricks are involved -- you'll implement them in jos!

* The file descriptor design:
  * FDs are a level of indirection
    - a process's real I/O environment is hidden in the kernel
    - preserved over fork and exec
    - separates I/O setup from use
    - imagine writefile(filename, offset, buf size)
  * FDs help make programs more general purpose: don't need special cases for
    files vs console vs pipe

* Philosophy: small set of conceptually simple calls that combine well
  e.g. fork(), open(), dup(), exec()
  command-line design has a similar approach
    ls | wc -l

* Why must kernel support pipes -- why not have sh simulate them, e.g.
  ls > tempfile ; wc -l < tempfile

* System call interface simple, just ints and char buffers.  why not have open()
  return a pointer reference to a kernel file object?

* The core UNIX system calls are ancient; have they held up well?
  yes; very successful
    and evolved well over many years
  history: design caters to command-line and s/w development
    system call interface is easy for programmers to use
    command-line users like named files, pipelines, &c
    important for development, debugging, server maintenance
  but the UNIX ideas are not perfect:
    programmer convenience is often not very valuable for system-call API
      programmers use libraries e.g. Python that hide sys call details
      apps may have little to do with files &c, e.g. on smartphone
    some UNIX abstractions aren't very efficient
      fork() for multi-GB process is very slow
      FDs hide specifics that may be important
        e.g. block size for on-disk files
        e.g. timing and size of network messages
  so there has been lots of work on alternate plans
    sometimes new system calls and abstractions for existing UNIX-like kernels
    sometimes entirely new approaches to what a kernel should do
  ask "why this way? wouldn't design X be better?"

## OS organization

* main goal: isolation

  * Processors provide user/kernel mode
     kernel mode: can execute "privileged" instructions
       e.g., setting kernel/user bit
     user mode: cannot execute privileged instructions

  * Operating system runs in kernel mode
    - kernel is "trusted"
      can set user/kernel bit
      direct hardware access
	  
  * Applications run in user mode
    - kernel sets up per-process isolated address space
    - system calls switch between user and kernel mode
      the application executes a special instruction to enter kernel
      hardware switches to kernel mode
      but only at an entry point specified by the kernel

* What to put in the kernel?

  * xv6 follows a traditional design: all of the OS runs in kernel mode
    - one big program with file system, drivers, &c
    - this design is called a monolithic kernel
    - kernel interface == system call interface
    - good: easy for subsystems to cooperate
      one cache shared by file system and virtual memory
    - bad: interactions are complex
      leads to bugs
      no isolation within kernel

  * microkernel design
    - many OS services run as ordinary user programs
      file system in a file server
    - kernel implements minimal mechanism to run services in user space
      processes with memory
      inter-process communication (IPC)
    - kernel interface != system call interface		
    - good: more isolation
    - bad: may be hard to get good performance

  * exokernel: no abstractions
    apps can use hardware semi-directly, but O/S isolates
    e.g. app can read/write own page table, but O/S audits
    e.g. app can read/write disk blocks, but O/S tracks block owners
    good: more flexibility for demanding applications
    jos will be a mix of microkernel and exokernel

* Can one have process isolation WITHOUT h/w-supported kernel/user mode?
  yes!
  see Singularity O/S, later in semester
  but h/w user/kernel mode is the most popular plan

Next lecture: isolation mechanisms and xv6's use of them

6.828 2016 Lecture 6: Virtual Memory
==

* plan:
  * address spaces
  * paging hardware
  * xv6 VM code
    *   case study
    *   finish lec 5
    *   homework sol

## Virtual memory overview

* today's problem:
  [user/kernel diagram]
  [memory view: diagram with user processes and kernel in memory]
  suppose the shell has a bug:
    sometimes it writes to a random memory address
  how can we keep it from wrecking the kernel?
    and from wrecking other processes?

* we want isolated address spaces
  each process has its own memory
  it can read and write its own memory
  it cannot read or write anything else
  challenge: 
    how to multiplex several memories over one physical memory?
	while maintaining isolation between memories

* xv6 and JOS uses x86's paging hardware to implement AS's
  ask questions! this material is important

* paging provides a level of indirection for addressing
  CPU -> MMU -> RAM
      VA     PA
  s/w can only ld/st to virtual addresses, not physical
  kernel tells MMU how to map each virtual address to a physical address
    MMU essentially has a table, indexed by va, yielding pa
    called a "page table"
  MMU can restrict what virtual addresses user code can use

* x86 maps 4-KB "pages"
  and aligned -- start on 4 KB boundaries
  thus page table index is top 20 bits of VA

* what is in a page table entry (PTE)?
  see [handout](x86_translation_and_registers.pdf)
  top 20 bits are top 20 bits of physical address
    "physical page number"
    MMU replaces top 20 of VA with PPN
  low 12 bits are flags
    Present, Writeable, &c

* where is the page table stored?
  in RAM -- MMU loads (and stores) PTEs
  o/s can read/write PTEs

* would it be reasonable for page table to just be an array of PTEs?
  how big is it?
  2^20 is a million
  32 bits per entry
  4 MB for a full page table -- pretty big on early machines
  would waste lots of memory for small programs!
    you only need mappings for a few hundred pages
    so the rest of the million entries would be there but not needed

* x86 uses a "two-level page table" to save space
  diagram
  pages of PTEs in RAM
  page directory (PD) in RAM
  PDE also contains 20-bit PPN -- of a page of 1024 PTEs
  1024 PDEs point to PTE pages
    each PTE page has 1024 PTEs -- so 1024*1024 PTEs in total
  PD entries can be invalid
    those PTE pages need not exist
    so a page table for a small address space can be small

* how does the mmu know where the page table is located in RAM?
  %cr3 holds phys address of PD
  PD holds phys address of PTE pages
  they can be anywhere in RAM -- need not be contiguous

* how does x86 paging hardware translate a va?
  need to find the right PTE
  %cr3 points to PA of PD
  top 10 bits index PD to get PA of PT
  next 10 bits index PT to get PTE
  PPN from PTE + low-12 from VA

* flags in PTE
  P, W, U
  xv6 uses U to forbid user from using kernel memory

* what if P bit not set? or store and W bit not set?
  "page fault"
  CPU saves registers, forces transfer to kernel
  trap.c in xv6 source
  kernel can just produce error, kill process
  or kernel can install a PTE, resume the process
    e.g. after loading the page of memory from disk

* Q: why mapping rather than e.g. base/bound?
  indirection allows paging h/w to solve many problems
  e.g. avoids fragmentation
  e.g. copy-on-write fork
  e.g. lazy allocation (home work for next lecture)
  many more techniques
  topic of next lecture
  
* Q: why use virtual memory in kernel?
  it is clearly good to have page tables for user processes
  but why have a page table for the kernel?
    could the kernel run with using only physical addresses?
  top-level answer: yes
    Singularity is an example kernel using phys addresses
	but, most standard kernels do use virtual addresses?
  why do standard kernels do so?
    some reasons are lame, some are better, none are fundamental
    - the hardware makes it difficult to turn it off
	    e.g. on entering a system call, one would have to disable VM
	- it can be convenient for the kernel to use user addresses
	  e.g. a user address passed to a system call
	  but, probably a bad idea: poor isolation between kernel/application
	- convenient if addresses are contiguous
	  say kernel has both 4Kbyte objects and 64Kbyte objects
      without page tables, we can easily have memory fragmentation
	  e.g., allocate 64K, allocate 4Kbyte, free the 64K, allocate 4Kbyte from the 64Kbyte
	  now a new 64Kbyte object cannot use the free 60Kbyte.
	- the kernel must run of a wide range of hardware
	  they may have different physical memory layouts

## Case study: xv6 use of the x86 paging hardware

* big picture of an xv6 address space -- one per process
  [diagram]
  0x00000000:0x80000000 -- user addresses below KERNBASE
  0x80000000:0x80100000 -- map low 1MB devices (for kernel)
  0x80100000:?          -- kernel instructions/data
  ?         :0x8E000000 -- 224 MB of DRAM mapped here
  0xFE000000:0x00000000 -- more memory-mapped devices
* where does xv6 map these regions, in phys mem?
<!--
 diagram from book: xv6-layout.eps
-->
  note double-mapping of user pages
* each process has its own address space
  and its own page table
  all processes have the same kernel (high memory) mappings
  kernel switches page tables (i.e. sets %cr3) when switching processes
* Q: why this address space arrangement?
  user virtual addresses start at zero
    of course user va 0 maps to different pa for each process
  2GB for user heap to grow contiguously
    but needn't have contiguous phys mem -- no fragmentation problem
  both kernel and user mapped -- easy to switch for syscall, interrupt
  kernel mapped at same place for all processes
    eases switching between processes
  easy for kernel to r/w user memory
    using user addresses, e.g. sys call arguments
  easy for kernel to r/w physical memory
    pa x mapped at va x+0x80000000
    we'll see this soon while manipulating page tables
* Q: what's the largest process this scheme can accommodate?
* Q: could we increase that by increasing/decreasing 0x80000000?
* Q: does the kernel have to map all of phys mem into its virtual address space?
* let's look at some xv6 virtual memory code
  terminology: virtual memory == address space / translation
  will help you w. next homework and labs

```c
start where Robert left off: first process

setup: CPUS=1, turn-off interrupts in lapic.c
b proc.c:297

p *p
Q: are these addresses virtual addresses

break into qemu: info pg (modified 6.828 qemu)

step into switchuvm

x/1024x p->pgdir
what is 0x0dfbc007?  (pde; see handout)
what is 0x0dfbc000?
what is 0x0dfbc000 + 0x8000000
what is there? (pte)
what is at 0x8dfbd000?
x x/i 0x8dfbd000 (first word of initcode.asm)

step passed lcr3

qemu: info pg 
```



* tracing and date system call

<!-- homework
syscall trace 
  syscall.c (HWSYS)
  return value in eax
  use STAB for printing out names
date
  usys.S
  syscall.c (HWDATE)
  argptr
-->


* a process calls sbrk(n) to ask for n more bytes of heap memory
  malloc() uses sbrk()
  each process has a size
    kernel adds new memory at process's end, increases size
  sbrk() allocates physical memory (RAM)
  maps it into the process's page table
  returns the starting address of the new memory

* sys_sbrk() in sysproc.c
<!---
   trace sbrk from user space
   just run ls (or any other cmd from shell)
   the new process forked by shell calls malloc for execcmd structure
   malloc.c calls sbrk
-->

* growproc() in proc.c
  proc->sz is the process's current size
  allocuvm() does most of the work
  switchuvm sets %cr3 with new page table
    also flushes some MMU caches so it will see new PTEs

* allocuvm() in vm.c
  why if(newsz >= KERNBASE) ?
  why PGROUNDUP?
  arguments to mappages()...



* where did this pgdir get setup?
  look at vm.c: setupkvm and inituvm
* mappages() in vm.c
  arguments are PD, va, size, pa, perm
  adds mappings from a range of va's to corresponding pa's
  rounds b/c some uses pass in non-page-aligned addresses
  for each page-aligned address in the range
    call walkpgdir to find address of PTE
      need the PTE's address (not just content) b/c we want to modify
    put the desired pa into the PTE
    mark PTE as valid w/ PTE_P
* diagram of PD &c, as following steps build it
* walkpgdir() in vm.c
  mimics how the paging h/w finds the PTE for an address
  refer to the handout
  PDX extracts top ten bits
  &pgdir[PDX(va)] is the address of the relevant PDE
  now *pde is the PDE
  if PTE_P
    the relevant page-table page already exists
    PTE_ADDR extracts the PPN from the PDE
    p2v() adds 0x80000000, since PTE holds physical address
  if not PTE_P
    alloc a page-table page
    fill in PDE with PPN -- thus v2p
  now the PTE we want is in the page-table page
    at offset PTX(va)
    which is 2nd 10 bits of va

```c
finish starting the first user process

return to gdb

(draw picture of kstack)
p /x p->tf
p /x *p->tf
p /x p->context
p /x p->context

b *0x0

swtch
x/8x $esp
forkret
x/19x $esp
info reg

step till user space:
x/i 0x0

step through use code
trap into kernel

x/19x $esp
```







6.828 2016 Lecture 7: using virtual memory
==

* plan: cool things you can do with vm
  - Better performance/efficiency
    e.g., one zero-filled page
	e.g., copy-on-write fork
  - New features
    e.g., memory-mapped files
  - JOS and VM
  - This lecture may generate ideas for last lab (final project)
  

<!--
  isolation: picture with walls

  return user space, until we hit first system call
    then switch to date homework
	
  date system call homework
    point out some of the walls:
	  U/K bit
        user cannot execute privileged instructions
		user enter kernel only through system calls
	    only kernel can load cr3
	  Page tables
	    no U bit on kernel pages
	  But sharing too:
	    Kernel can read/write user memory
	    Requires kernel checks arguments of system call
-->

* virtual memory: several views
  * primary purpose: isolation
    each process has its own address space
  * Virtual memory provides a level-of-indirection
    provides kernel with opportunity to do cool stuff

* lazy/on-demand page allocation
  * sbrk() is old fashioned;
    it asks application to "predict" how much memory they need
	difficult for applications to predict how much memory they need in advance
	sbrk allocates memory that may never be used.
  * moderns OSes allocate memory lazily
    allocate physical memory when application needs it
  * HW solution
    <!---
	  draw xv6 user-part of address space 
	  demo solution; breakpoint right before mappages in trap.c
      explain page faults
	-->

<!--
	xv6 memlayout discussion
	
  user virtual addresses start at zero
    of course user va 0 maps to different pa for each process
  2GB for user heap to grow contiguously
    but needn't have contiguous phys mem -- no fragmentation problem
  both kernel and user mapped -- easy to switch for syscall, interrupt
  kernel mapped at same place for all processes
    eases switching between processes
  easy for kernel to r/w user memory
    using user addresses, e.g. sys call arguments
  easy for kernel to r/w physical memory
    pa x mapped at va x+0x80000000
    we'll see this soon while manipulating page tables
	
  lame part: user stack
    also, initcode and date (different AS layout)
	but convenient to check if an address is valid (va < p->size)
	
  why is kernel using vm?
-->

* Step back: class perspective
  - There is no one best way to design an OS
    Many OSes use VM, but you don't have to
  - Xv6 and JOS present examples of OS designs
    They lack many features of sophisticated designs
    In fact, they are pretty lame compared to a real OS
	Yet, still quite complex
  - Our goal: to teach you the key ideas so that you can extrapolate
    Xv6 and JOS are minimal design to expose key ideas
	You should be able to make them better
	You should be able to dive into Linux and find your way

* guard page to protect against stack overflow
  * put a non-mapped page below user stack
    if stack overflows, application will see page fault
  * allocate more stack when application runs off stack into guard page 
    <!---
	  draw xv6 user-part of address space 
      compile with -O so the compiler doesn't optimize the tail recursion
	  demo stackoverflow 
        set breakpoint at g
  	    run stackoverflow 
	    look at $esp
	    look at pg info at qemu console
	    note page has no U bit
	-->

* one zero-filled page
  * kernel often fills a page with zeros
  * idea: memset *one* page with zeros
    map that page copy-on-write when kernel needs zero-filled page
    on write make copy of page and map it read/write in app address space

* share kernel page tables in xv6
  * observation:
    kvmalloc() allocates new pages for kernel page table for each process
    but all processes have the same kernel page table
  * idea: modify kvmalloc()/freevm() to share kernel page table
    <!---
	  demo HWKVM
	-->

* copy-on-write fork
  * observation:
    xv6 fork copies all pages from parent (see fork())
    but fork is often immediately followed by exec
  * idea: share address space between parent and child
    modify fork() to map pages copy-on-write (use extra available system bits in PTEs and PDEs)
    on page fault, make copy of page and map it read/write

* demand paging
  * observation: exec loads the complete file into memory (see exec.c)
    expensive: takes time to do so (e.g., file is stored on a slow disk)
    unnecessary: maybe not the whole file will be used
  * idea: load pages from the file on demand
    allocate page table entries, but mark them on-demand
    on fault, read the page in from the file and update page table entry
  * challenge: file larger than physical memory (see next idea)

* use virtual memory larger than physical memory
  * observation: application may need more memory than there is physical memory
  * idea: store less-frequently used parts of the address space on disk
    page-in and page-out pages of the address address space transparently
  * works when working sets fits in physical memory

* memory-mapped files
  * idea: allow access to files using load and store
    can easily read and writes part of a file
    e.g., don't have to change offset using lseek system call
  * page-in pages of a file on demand
    when memory is full, page-out pages of a file that are not frequently used

* shared virtual memory
  * idea: allow processes on different machines to share virtual memory
    gives the illusion of physical shared memory, across a network
  * replicate pages that are only read
  * invalidate copies on write
  
* JOS and virtual memory
  * layout: [picture](l-josmem.html)

  * UVPT trick (lab 4)
    recursively map PD at 0x3BD
      virtual address of PD is (0x3BD<<22)|(0x3BD<<12)
    if we want to find pte for virtual page n, compute
		pde_t uvpt[n], where uvpt is (0x3BD << 22) 
        = uvpt + n * 4 (because pdt is a word)
	    = (0x3BD << 22) | (top 10 bits of n) | (bottom 10 bits of n) << 2
		= 10 | 10 | 12
    for example, uvpt[0] is address (0x3BD << 22), following the pointers gives us
	the first entry in the page directory, which points to the first page table, which
	we index with 0, which gives us pte 0

    simpler than pgdirwalk()?
    
  * user-level copy-on-write fork (lab4)
    JOS propagates page faults to user space
    user programs can play similar VM tricks as kernel!
    you will do user-level copy-on-write fork





# 6.828 2017 Lecture 8: Isolation mechanisms

Today:
  user/kernel isolation
  xv6 system call as case study

* Multiple processes drive the key requirements:
  multiplexing
  isolation
  interaction/sharing

* Isolation is often the most constraining requirement.

* What is isolation?
  enforced separation to contain effects of failures
  the process is the usual unit of isolation
  prevent process X from wrecking or spying on process Y
    r/w memory, use 100% of CPU, change FDs, &c
  prevent a process from interfering with the operating system 
  in the face of malice as well as bugs
    a bad process may try to trick the h/w or kernel

* the kernel uses hardware mechanisms as part of process isolation:
  user/kernel mode flag
  address spaces
  timeslicing
  system call interface

* the hardware user/kernel mode flag
  controls whether instructions can access privileged h/w
  called CPL on the x86, bottom two bits of %cs register
    CPL=0 -- kernel mode -- privileged
    CPL=3 -- user mode -- no privilege
  x86 CPL protects many processor registers relevant to isolation
    I/O port accesses
    control register accesses (eflags, %cs4, ...)
      including %cs itself
    affects memory access permissions, but indirectly
    the kernel must set all this up correctly
  every serious microprocessor has some kind of user/kernel flag
  
* how to do a system call -- switching CPL
  Q: would this be an OK design for user programs to make a system call:
    set CPL=0
    jmp sys_open
    bad: user-specified instructions with CPL=0
  Q: how about a combined instruction that sets CPL=0,
    but *requires* an immediate jump to someplace in the kernel?
    bad: user might jump somewhere awkward in the kernel
  the x86 answer:
    there are only a few permissible kernel entry points ("vectors")
    INT instruction sets CPL=0 and jumps to an entry point
    but user code can't otherwise modify CPL or jump anywhere else in kernel
  system call return sets CPL=3 before returning to user code
    also a combined instruction (can't separately set CPL and jmp)

* the result: well-defined notion of user vs kernel
  either CPL=3 and executing user code
  or CPL=0 and executing from entry point in kernel code
  not:
    CPL=0 and executing user code
    CPL=0 and executing anywhere in kernel the user pleases

* how to isolate process memory?
  idea: "address space"
  give each process some memory it can access
    for its code, variables, heap, stack
  prevent it from accessing other memory (kernel or other processes)

* how to create isolated address spaces?
  xv6 uses x86 "paging hardware" in the memory management unit (MMU)
  MMU translates (or "maps") every address issued by program
    CPU -> MMU -> RAM
            |
         pagetable
    VA -> PA
    MMU translates all memory references: user and kernel, instructions and data
    instructions use only VAs, never PAs
  kernel sets up a different page table for each process
    each process's page table allows access only to that process's RAM
  
### Let's look at how xv6 system calls are implemented

xv6 process/stack diagram:
  user process ; kernel thread
  user stack ; kernel stack
  two mechanisms:
    switch between user/kernel
    switch between kernel threads
  trap frame
  kernel function calls...
  struct context

* simplified xv6 user/kernel virtual address-space setup
  FFFFFFFF:
            ...
  80000000: kernel
            user stack
            user data
  00000000: user instructions
  kernel configures MMU to give user code access only to lower half
  separate address space for each process
    but kernel (high) mappings are the same for every process

system call starting point: 
  executing in user space, sh writing its prompt
  sh.asm, write() library function
  break *0xd42
  x/3i
    0x10 in eax is the system call number for write
  info reg
    cs=0x1b, B=1011 -- CPL=3 => user mode
    esp and eip are low addresses -- user virtual addresses
  x/4x $esp
    ebf is return address -- in printf
    2 is fd
    0x3f7a is buffer on the stack
    1 is count
    i.e. write(2, 0x3f7a, 1)
  x/c 0x3f7a

INT instruction, kernel entry
  stepi
  info reg
    cs=0x8 -- CPL=3 => kernel mode
    note INT changed eip and esp to high kernel addresses
  where is eip?
    at a kernel-supplied vector -- only place user can go
    so user program can't jump to random places in kernel with CPL=0
  x/6wx $esp
    INT saved a few user registers
    err, eip, cs, eflags, esp, ss
  why did INT save just these registers?
    they are the ones that INT overwrites
  what INT did:
    switched to current process's kernel stack
    saved some user registers on kernel stack
    set CPL=0
    start executing at kernel-supplied "vector"
  where did esp come from?
    kernel told h/w what kernel stack to use when creating process

Q: why does INT bother saving the user state?
   how much state should be saved?
   transparency vs speed

saving the rest of the user registers on the kernel stack
  trapasm.S alltraps
  pushal pushes 8 registers: eax .. edi
  x/19x $esp
  19 words at top of kernel stack:
    ss
    esp
    eflags
    cs
    eip
    err    -- INT saved from here up
    trapno
    ds
    es
    fs
    gs
    eax..edi
  will eventually be restored, when system call returns
  meanwhile the kernel C code sometimes needs to read/write saved values
  struct trapframe in x86.h

Q: why are user registers saved on the kernel stack?
   why not save them on the user stack?

entering kernel C code
  the pushl %esp creates an argument for trap(struct trapframe *tf)
  now we're in trap() in trap.c
  print tf
  print *tf

kernel system call handling
  device interrupts and faults also enter trap()
  trapno == T_SYSCALL
  myproc()
  struct proc in proc.h
  myproc()->tf -- so syscall() can get at call # and arguments
  syscall() in syscall.c
    looks at tf->eax to find out which system call
  SYS_write in syscalls[] maps to sys_write
  sys_write() in sysfile.c
  arg*() read write(fd,buf,n) arguments from the user stack
  argint() in syscall.c
    proc->tf->esp + xxx

restoring user registers
  syscall() sets tf->eax to return value
  back to trap()
  finish -- returns to trapasm.S
  info reg -- still in kernel, registers overwritten by kernel code
  stepi to iret
  info reg
    most registers hold restored user values
    eax has write() return value of 1
    esp, eip, cs still have kernel values
  x/5x $esp
    saved user state: eip, cs, eflags, esp, ss
  IRET pops those user registers from the stack
    and thereby re-enters user space with CPL=3

Q: do we really need IRET?
   could we use ordinary instructions to restore the registers?
   could IRET be simpler?

back to user space
  stepi
  info reg

* Summary
 intricate design for User/Kernel transition
    how bad is a bug in this design?
 kernel must take adversarial view of user process
   doesn't trust user stack
   checks arguments
 page table confines what memory user program can read/write
   next lecture





# 6.828 2018 Lecture 8: Interrupts, System calls, and Exceptions

the general topic: hardware wants attention now!
  software must set aside current work and respond

why does hw want attention now?
  MMU cannot translate address
  User program divides by zero
  User program wants to execute kernel code (INT)
  Network hardware wants to deliver a packet
  Timer hardware wants to deliver a "tick"
  kernel CPU-to-CPU communication, e.g. to flush TLB (IPI)

these "traps" fall broadly speaking in 3 classes:
  Exceptions (page fault, divide by zero)
  System calls (INT, intended exception)
  Interrupts (devices want attention)
  (Warning: terminology isn't used consistently)

where do device interrupts come from?
  diagram:
    CPUs, LAPICs, IOAPIC, devices
    data bus
    interrupt bus
  the interrupt tells the kernel the device hardware wants attention
  the driver (in the kernel) knows how to tell the device to do things
  often the interrupt handler calls the relevant driver
    but other arrangements are possible (schedule a thread; poll)

how does trap() know which device interrupted?
  i.e. where did tf->trapno == T_IRQ0 + IRQ_TIMER come from?
  kernel tells LAPIC/IOAPIC what vector number to use, e.g. timer is vector 32
    page faults &c also have vectors (as we saw in last lecture)
    LAPIC / IOAPIC are standard pieces of PC hardware
    one LAPIC per CPU
  IDT associates an instruction address with each vector number
    IDT format is defined by Intel, configured by kernel
  each vector jumps to alltraps
  CPU sends many kinds of traps through IDT
    low 32 IDT entries have special fixed meaning
  xv6 sets up system calls (IRQ) to use IDT entry 64 (0x40)
  the point: the vector number reveals the source of the interrupt

diagram:
  IRQ or trap, IDT table, vectors, alltraps
  IDT:
    0: divide by zero
    13: general protection
    14: page fault
    32-255: device IRQs
    32: timer
    33: keyboard
    46: IDE
    64: INT

let's look at how xv6 sets up the interrupt vector machinery
  lapic.c / lapicinit() -- tells LAPIC hardware to use vector 32 for timer
  trap.c / tvinit() -- initializes IDT, so entry i points to code at vector[i]
    this is mostly purely mechanical, IDT entries correspond blindly to vectors
    BUT T_SYSCALL's 1 (vs 0) tells CPU to leave interrupts enabled during system calls
    but not during device interrupts
    Q: why allow interrupts during system calls?
    Q: why disable interrupts during interrupt handling?
  vectors.S (generated by vectors.pl)
    first push fakes "error" slot in trapframe, since h/w doesn't push for some traps
    second push is just the vector number
      this shows up in trapframe as tf->trapno

how does the hardware know what stack to use for an interrupt?
  when it switches from user space to the kernel
  hardware-defined TSS (task state segment) lets kernel configure CPU 
    one per CPU
    so each CPU can run a different process, take traps on different stacks
  proc.c / scheduler()
    one per CPU
  vm.c / switchuvm()
    tells CPU what kernel stack to use
    tells kernel what page table to use

Q: what eip should the CPU save when trapping to the kernel?
   eip of the instruction that was executing?
   eip of the next instruction?
   suppose the trap is a page fault?

Let's talk about homework, which involves:
  interrupts + system calls
  challenges:
    get it to work at all
    maintain isolation (unfortunately, not easy way to test!)

alarmtest.c
  alarm(10, periodic)
  asks kernel to call periodic() every 10 "ticks" in this process
  that is, every 10 ticks of CPU time that this process consumes
  three pieces:
    add a new system call
    count ticks as the program runs (timer interrupt)
    kernel "upcall" to periodic()
  the call to periodic() is a simplified UNIX signal

glue for a new system call  (like date homework)
  syscall.h: #define SYS_alarm 22
  usys.S: SYSCALL(alarm)
    alarmtest.asm -- mov $0x16,%eax -- 0x16 is SYS_alarm
  syscall.c syscalls[] table
  sysproc.c sys_alarm()

break sys_alarm (like date homework)
  where
  how did syscall know which system call?
    trapframe, on kernel stack, has saved user eax
    print myproc()->tf->eax
  where does sys_alarm find the arguments, ticks and handler?
    on the user stack
    x/4x myproc()->tf->esp
  does the handler value make sense? look in alarmtest.asm

now we need to take some action whenever the timer h/w interrupts
  decrement ticksleft
  if expired
    upcall to handler (periodic())
    reset ticksleft

device interrupts arrive just like INT and pagefault
  h/w pushes esp and eip on kernel stack
  s/w saves other registers, into a trapframe
  vector, alltraps, trap()

timer interrupts served by IRQ_TIMER case in trap()
  original IRQ_TIMER task is to keep track of wall-clock time, in ticks

execute to trap without an implementation
  break vector32
  where
  print/x tf->eip
  print/x tf->esp
  x/4x tf->esp

what was the user program doing at this point?
  tf->eip in alarmtest.asm
  user code could have been interrupted anywhere
    so we can't rely on anything about the user stack
    and we need to restore registers exactly, since program didn't save anything

Q: how to arrange for upcall to alarm handler?
   call myproc()->alarmhandler() ?
   tf->eip = myproc()->alarmhandler ?
Q: how to ensure handler returns to interrupted user code?

add our code...
run alarmtest without gdb

let's run with gdb
  list trap to find breakpoint
  print/x tf->eip before assignment
  print/x tf->eip after assignment
  break *0x74
  c
  info reg
  will it return somewhere reasonable in alarmtest.asm?
  x/4x $esp

Q: what's the security problem in my new trap() code?

Q: what if trap() directly called alarmhandler()?
   it's a bad idea
   but what exactly would go wrong?
   let's try it
     it doesn't crash!
     but it doesn't print alarm! either. why not?
     fetchint...
   apparently it gets back to user space (to print .) -- how?
     program, timer trap, alarmhandler(), INT, sys_write("alarm!"), return...
     stack diagram

it is disturbing how close this came to working!
  why can kernel code directly jump to user instructions?
  why can user instructions modify the kernel stack?
  why do system calls (INT) work from the kernel?
  none of these are intended properties of xv6!
  the x86 h/w does *not* directly provide isolation
    x86 has many separate features (page table, INT, &c)
    it's possible to configure these features to enforce isolation
    but isolation is not the default!

Q: what happens if just tf->eip = alarmhandler, but don't push old eip?
   let's try it
   user stack diagram

Q: what if trap() didn't check for CPL 3?
   let's try it -- seems to work!
   how could tf->cs&3 == 0 ever arise from alarmtest?
   let's force the situation with (tf->cs&3)==0
     and making alarmtest run forever
     unexpected trap 14 from cpu 0 eip 801067cb (cr2=0x801050cf)
   what is eip 0x801067cb in kernel.asm?
     tf->esp = tf->eip in trap().
   what happened?
     it was a CPL=0 to CPL=0 interrupt
     so the h/w didn't switch stacks
     so it didn't save %esp
     so tf->esp contains garbage
     (see comment at end of trapframe in x86.h)
   the larger point is that interrupts can occur while in the kernel (in xv6, not JOS)

Q: what will happen if user-supplied alarm handler fn points into the kernel?
   (with the correct trap() code)

Q: what if another timer interrupt goes off while in user handler?
   works, but confusing, and will eventually run out of user stack
   maybe kernel shouldn't re-start timer until handler function finishes

Q: is it a problem if periodic() modifies registers?
   how could we arrange to restore registers before returning?

Let's step back and talk about interrupts a bit more generally

Interrupts introduces concurrency
  Other code runs between my code
  For example, my code is
    1. add %eax, %ebx
    2. add %ebx, %exc
  Q: Might other code run between 1 and 2?
    Yes!
  For user code maybe not that bad
    Kernel will resume user code in in the same state
    But, alarmtest should be aware that periodic() could run between any two instructions
  For kernel code could be difficult
    Interrupt handler may update state that is observable by my code
    my code:               interrupt:
      %eax = 0
      if %eax = 0 then        %eax = 1
        f()
     f() may be executed or may not be executed!
    To make a block of code "atomic", turn off interrupts
      cli()
      sti()
  Our first glimps of "concurrency"
    We'll get back to this when discussing locking

Interrupt evolution
  Interrupts used to be relatively fast; now they are slow
    old approach: every event causes an interrupt, simple h/w, smart s/w
    new approach: h/w completes lots of work before interrupting
  Some devices generate events faster than one per microsecond
    e.g. gigabit ethernet can deliver 1.5 million small packets / second
  An interrupt takes on the order of a microsecond
    save/restore state
    cache misses
    what to do if interrupt comes in faster than 1 per microsecond?

Polling: another way of interacting with devices
  Processor spins until device wants attention
    Wastes processor cycles if device is slow
  But inexpensive if device is fast
    No saving of registers etc.
  If events are always waiting, no need to keep alerting the software

Polling versus interrupts
  Polling rather than interrupting, for high-rate devices
  Interrupt for low-rate devices, e.g. keyboard
    constant polling would waste CPU
  Switch between polling and interrupting automatically
    interrupt when rate is low (and polling would waste CPU cycles)
    poll when rate is high  (and interrupting would waste CPU cycles)
  Faster forwarding of interrupts to user space
    for page faults and user-handled devices
    h/w delivers directly to user, w/o kernel intervention?
    faster forwarding path through kernel?
  We will be seeing many of these topics later in the course



# 6.828 2016 : Locking

### Why talk about locking?

  apps want to use multi-core processors for parallel speed-up
  so kernel must deal with parallel system calls
  and thus parallel access to kernel data (buffer cache, processes, &c)
  locks help with correct sharing of data
  locks can limit parallel speedup

Locking homework
  recall: ph.c, multi-threaded hash table, put(), get()
  vi ph0.c
  Q: why run ph.c on multiple cores?
     diagram: CPUs, bus, RAM
     assumption: CPU is bottleneck, can divide work between CPUs
  Q: can we beat single-core put() time? single-core get() time?
     ./ph0 1
  Q: how to measure parallel speedup?
     ./ph0 2
     twice as much work per unit time!
  Q: where are the missing keys?
  Q: specific scenario?
     diagram...
     table[0] = 15
     concurrent put(5), put(10)
     both insert()s allocate new entry, point next to 15
     both set table[0] to their new entry
     last inserter wins, other one is lost!
     called a "lost update"; example of a "race"
     race = concurrent accesses; at least one write
  Q: where to put the lock/unlock?
  Q: one lock covering the whole hash table?
     why? why not?
     called a "big" or "coarse-grained" lock
     ./ph1 2
     faster? slower? why?
  Q: one lock per table[] entry?
     this lock is "finer grained"
     why might this be good?
     ./ph2 2
     faster? slower? why?
     what might be harder with per-bucket locks?
     will we get a good speedup with 10 cores? NBUCKET=5...
  Q: one lock per struct entry, protecting the next pointer?
     why? why not?
  Q: does get() need to lock?
  Q: does get() need to lock if there are concurrent put()s?
     it's a race; but is it incorrect?

The lock abstraction:
  lock l
  acquire(l)
    x = x + 1 -- "critical section"
  release(l)
  a lock is itself an object
  if multiple threads call acquire(l)
    only one will return right away
    the others will wait for release() -- "block"
  a program typically has lots of data, lots of locks
    if different threads use different data,
    then they likely hold different locks,
    so they can execute in parallel -- get more work done.
  note that lock l is not specifically tied to data x
    the programmer has a plan for the correspondence

A conservative rule to decide when you need to lock:
  any time two threads use a memory location, and at least one is a write
  don't touch shared data unless you hold the right lock!
  (too strict: program logic may sometimes rule out sharing; lock-free)
  (too loose: printf(); not always simple lock/data correspondence)

Could locking be automatic?
  perhaps the language could associate a lock with every data object
    compiler adds acquire/release around every use
    less room for programmer to forget!
  that idea is often too rigid:
    rename("d1/x", "d2/y"):
      lock d1, erase x, unlock d1
      lock d2, add y, unlock d2
    problem: the file didn't exist for a while!
      rename() should be atomic
        other system calls should see before, or after, not in between
      otherwise too hard to write programs
    we need:
      lock d1 ; lock d2
      erase x, add y
      unlock d2; unlock d1
  that is, programmer often needs explicit control over
    the region of code during which a lock is held
    in order to hide awkward intermediate states

Ways to think about what locks achieve
  locks help avoid lost updates
  locks help you create atomic multi-step operations -- hide intermediate states
  locks help operations maintain invariants on a data structure
    assume the invariants are true at start of operation
    operation uses locks to hide temporary violation of invariants
    operation restores invariants before releasing locks

Problem: deadlock
  notice rename() held two locks
  what if:
    core A              core B
    rename(d1/x, d2/y)  rename(d2/a, d1/b)
      lock d1             lock d2
      lock d2 ...         lock d1 ...
  solution:
    programmer works out an order for all locks
    all code must acquire locks in that order
    i.e. predict locks, sort, acquire -- complex!

Locks versus modularity
  locks make it hard to hide details inside modules
  to avoid deadlock, I need to know locks acquired by functions I call
  and I may need to acquire them before calling, even if I don't use them
  i.e. locks are often not the private business of individual modules

How to think about where to place locks?
  here's a simple plan for new code
  1. write module to be correct under serial execution
     i.e. assuming one CPU, one thread
     insert(){ e->next = l; l = e; }
     but not correct if executed in parallel
  2. add locks to FORCE serial execution
     since acquire/release allows execution by only one CPU at a time
    the point:

    it's easier for programmers to reason about serial code
    locks can cause your serial code to be correct despite parallelism

What about performance?
  after all, we're probably locking as part of a plan to get parallel speedup

Locks and parallelism
  locks *prevent* parallel execution
  to get parallelism, you often need to split up data and locks
    in a way that lets each core use different data and different locks
    "fine grained locks"
  choosing best split of data/locks is a design challenge
    whole ph.c table; each table[] row; each entry
    whole FS; directory/file; disk block
    whole kernel; each subsystem; each object
  you may need to re-design code to make it work well in parallel
    example: break single free memory list into per-core free lists
      helps if threads were waiting a lot on lock for single free list
    such re-writes can require a lot of work!

Lock granularity advice
  start with big locks, e.g. one lock protecting entire module
    less deadlock since less opportunity to hold two locks
    less reasoning about invariants/atomicity required
  measure to see if there's a problem
    big locks are often enough -- maybe little time spent in that module
  re-design for fine-grained locking only if you have to

Let's look at locking in xv6.

A typical use of locks: ide.c
  typical of many O/S's device driver arrangements
  diagram:
    user processes, kernel, FS, iderw, append to disk queue
    IDE disk hardware
    ideintr
  sources of concurrency: processes, interrupt
  only one lock in ide.c: idelock -- fairly coarse-grained
  iderw() -- what does idelock protect?
    1. no races in idequeue operations
    2. if queue not empty, IDE h/w is executing head of queue
    3. no concurrent access to IDE registers
  ideintr() -- interrupt handler
    acquires lock -- might have to wait at interrupt level!
    uses idequeue (1)
    hands next queued request to IDE h/w (2)
    touches IDE h/w registers (3)

How to implement locks?
  why not:
    struct lock { int locked; }
    acquire(l) {
      while(1){
        if(l->locked == 0){ // A
          l->locked = 1;    // B
          return;
        }
      }
    }
  oops: race between lines A and B
  how can we do A and B atomically?

Atomic exchange instruction:
  mov $1, %eax
  xchg %eax, addr
  does this in hardware:
    lock addr globally (other cores cannot use it)
    temp = *addr
    *addr = %eax
    %eax = temp
    unlock addr
  x86 h/w provides a notion of locking a memory location
    different CPUs have had different implementations
    diagram: cores, bus, RAM, lock thing
    so we are really pushing the problem down to the hardware
    h/w implements at granularity of cache-line or entire bus
  memory lock forces concurrent xchg's to run one at a time, not interleaved

Now:
  acquire(l){
    while(1){
      if(xchg(&l->locked, 1) == 0){
        break
      }
    }
  }
  if l->locked was already 1, xchg sets to 1 (again), returns 1,
    and the loop continues to spin
  if l->locked was 0, at most one xchg will see the 0; it will set
    it to 1 and return 0; other xchgs will return 1
  this is a "spin lock", since waiting cores "spin" in acquire loop

Look at xv6 spinlock implementation
  spinlock.h -- you can see "locked" member of struct lock
  spinlock.c / acquire():
    see while-loop and xchg() call
    what is the pushcli() about?
      why disable interrupts?
  release():
    sets lk->locked = 0
    and re-enables interrupts

Detail: memory read/write ordering
  suppose two cores use a lock to guard a counter, x
  and we have a naive lock implementation
  Core A:          Core B:
    locked = 1
    x = x + 1      while(locked == 1)
    locked = 0       ...
                   locked = 1
                   x = x + 1
                   locked = 0
  the compiler AND the CPU re-order memory accesses
    i.e. they do not obey the source program's order of memory references
    e.g. the compiler might generate this code for core A:
      locked = 1
      locked = 0
      x = x + 1
      i.e. move the increment outside the critical section!
    the legal behaviors are called the "memory model"
  release()'s call to __sync_synchronize() prevents re-order
    compiler won't move a memory reference past a __sync_synchronize()
    and (may) issue "memory barrier" instruction to tell the CPU
  acquire()'s call to xchg() has a similar effect:
    intel promises not to re-order past xchg instruction
    some junk in x86.h xchg() tells C compiler not to delete or re-order
      (volatile asm says don't delete, "m" says no re-order)
  if you use locks, you don't need to understand the memory ordering rules
    you need them if you want to write exotic "lock-free" code

Why spin locks?
  don't they waste CPU while waiting?
  why not give up the CPU and switch to another process, let it run?
  what if holding thread needs to run; shouldn't waiting thread yield CPU?
  spin lock guidelines:
    hold spin locks for very short times
    don't yield CPU while holding a spin lock
  systems often provide "blocking" locks for longer critical sections
    waiting threads yield the CPU
    but overheads are typically higher
    you'll see some xv6 blocking schemes later

Advice:
  don't share if you don't have to
  start with a few coarse-grained locks
  instrument your code -- which locks are preventing parallelism?
  use fine-grained locks only as needed for parallel performance
  use an automated race detector



```

```





```

```

```

```